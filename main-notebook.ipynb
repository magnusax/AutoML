{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys; sys.path.append(os.getcwd()+'/src')\n",
    "from importlib import reload\n",
    "import class_handler as classes; reload(classes)\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data to play with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64) (1797,)\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.datasets import load_breast_cancer\n",
    "#x, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "import sklearn\n",
    "x, y = sklearn.datasets.load_digits(return_X_y=True)\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1437, 64) (1437,) (360, 64) (360,)\n"
     ]
    }
   ],
   "source": [
    "# Lets split into train and test\n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]\n",
      " [1 0 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13.815510557964274"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "\n",
    "y_true = [0, 1, 2, 3, 1]\n",
    "y_pred = [1, 1, 2, 3, 0]\n",
    "y_pred = lb.fit_transform(np.array(y_pred))\n",
    "print(y_pred)\n",
    "log_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.array([1, 1, 2, 3, 0])\n",
    "len(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "catched!!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-ab892104b62b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEnsembleBaseClassifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'catched!!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0minstance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0minstance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "from base import EnsembleBaseClassifier\n",
    "module_name = 'adaboost'\n",
    "algorithm_name = 'MetaAdaBoostClassifierAlgorithm'\n",
    "from importlib import reload, import_module               \n",
    "\n",
    "try:\n",
    "    module = import_module(module_name)\n",
    "except:\n",
    "    raise ValueError(\"Could not import module '%s'\" % module_name)                \n",
    "algorithm = getattr(module, algorithm_name)\n",
    "\n",
    "# check if 'algorithm' is a subclass of 'EnsembleBaseClassifier' \n",
    "# and set property 'base_estimator' if so\n",
    "if issubclass(algorithm, EnsembleBaseClassifier):\n",
    "    print('catched!!')\n",
    "    instance = algorithm(base_estimator=self.base_estimator)\n",
    "else:\n",
    "    instance = algorithm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import adaboost; reload(adaboost)\n",
    "from adaboost import MetaAdaBoostClassifierAlgorithm \n",
    "superclass = MetaAdaBoostClassifierAlgorithm().__class__.__base__.__base__\n",
    "superclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load using new API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized classifiers:\n",
      "\tadaboost\n",
      "\tnearestneigbors\n",
      "\tlogreg\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "nearestneigbors\n",
      "logreg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': None,\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'max_iter': 100,\n",
       " 'multi_class': 'ovr',\n",
       " 'n_jobs': 1,\n",
       " 'penalty': 'l2',\n",
       " 'random_state': None,\n",
       " 'solver': 'liblinear',\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import classifiers\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import class_handler as metawrapper; reload(metawrapper)\n",
    "clfs = metawrapper.MetaWrapperClassifier(verbose=1, method='complete', base_estimator=KNeighborsClassifier())\n",
    "for n, clf in clfs.clf:\n",
    "    try:\n",
    "        print(clf.base_estimator)\n",
    "    except:\n",
    "        print(clf.name)\n",
    "clf.estimator.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clfs.fit_classifiers(X_train, y_train, n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_pred = clfs.predict_classifiers(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaulate performance on training data\n",
    "print(\"Training scores:\")\n",
    "train_scores = clfs.classifier_performance(y_train_pred, y_train, metric='accuracy')\n",
    "# Evaluate overfitting\n",
    "print(\"Test scores:\")\n",
    "test_scores = clfs.classifier_performance(clfs.predict_classifiers(X_test), y_test, metric='accuracy')\n",
    "# AdaBoost clearly overfits this dataset while LogiReg and KNN seem ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import visualize; reload(visualize)\n",
    "from visualize import Visualizer\n",
    "# This method expects a list of 2-tuples\n",
    "vis = Visualizer()\n",
    "vis.show_performance([tuple([x,y]) for x,y,_ in test_scores], fig_size=(12,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "ll = [('a',1),('b',2),('c',3)]\n",
    "idx = np.random.choice(len(ll), 1, replace=False)\n",
    "sample = []\n",
    "for id in idx:\n",
    "    sample.append(ll[id])\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "# Import classifiers\n",
    "import class_handler as classes; reload(classes)\n",
    "clfs = classes.Classifiers(verbose=1, method='complete')\n",
    "np.random.seed(1)\n",
    "optims = clfs.optimize_classifiers(X_train, y_train, n_iter=100, n_jobs=-1, random_state=15)\n",
    "\n",
    "print()\n",
    "for name, clf in optims:\n",
    "    print(\"Training score: \", name, accuracy_score(clf.predict(X_train), y_train))\n",
    "    print(\"Test score: \", name, accuracy_score(clf.predict(X_test), y_test)) \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import class_handler as classes; reload(classes)\n",
    "clfs = classes.Classifiers(verbose=1, method='chosen', clf=['logreg'])\n",
    "# Assume now that you only want to tune C-parameter in LogisticRegression() classifier:\n",
    "for name, classifier in clfs.clf:\n",
    "    if isinstance(classifier.estimator, type(LogisticRegression())):\n",
    "        print(\"Lorem Ipsum!!!!!!\")\n",
    "        for p in ['C', 'fit_intercept']:\n",
    "            classifier.cv_params_to_tune.append(p)\n",
    "        print( classifier.cv_params_to_tune )\n",
    "        assert set(classifier.cv_params_to_tune).issubset(set(classifier.cv_params.keys())), \"[%s]\" % \", \".join(classifier.cv_params.keys())\n",
    "\n",
    "# Now we can call optimize_classifiers \n",
    "for cv in [3,5,10]:\n",
    "    np.random.seed(cv)\n",
    "    print(\"cv = \",cv)\n",
    "    optims = clfs.optimize_classifiers(X_train, y_train, n_iter=100, cv=cv, n_jobs=-1)\n",
    "    print()\n",
    "    for name, clf in optims:\n",
    "        print(\"Training score: \", name, accuracy_score(clf.predict(X_train), y_train))\n",
    "        print(\"Test score: \", name, accuracy_score(clf.predict(X_test), y_test)) \n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base class for algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import adaboost; reload(adaboost)\n",
    "from adaboost import MetaAdaBoostClassifierAlgorithm\n",
    "ada = MetaAdaBoostClassifierAlgorithm(random_state=0, base_estimator='logreg')\n",
    "print(ada.get_info())\n",
    "ada.cv_param_dist = ada._param_dist_dict()\n",
    "print(ada.cv_param_dist)\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "param_dist = ada.cv_param_dist\n",
    "estimator = ada.estimator\n",
    "\n",
    "# IT SEEMS THAT THE LOGISTIC REGRESSION CLASSIFIER TENDS TO OVERFIT...SHOULD LOWER n_iter IF USING THAT ALGO.\n",
    "grid_search = RandomizedSearchCV(estimator, param_distributions=param_dist, n_iter=1, scoring='neg_log_loss',\n",
    "                       cv=10, n_jobs=-1, verbose=0, error_score=0, return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best mean score: %.4f\" % (grid_search.best_score_))\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy on test set: %.4f\" % accuracy_score(y_test, grid_search.best_estimator_.predict(X_test))) \n",
    "\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with nearest neighbors algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nearest_neighbors; reload(nearest_neighbors)\n",
    "from nearest_neighbors import MetaKNearestNeighborClassifierAlgorithm\n",
    "knn = MetaKNearestNeighborClassifierAlgorithm()\n",
    "print(knn.get_info())\n",
    "knn.cv_param_dist = knn._param_dist_dict()\n",
    "print(type(knn.cv_param_dist))\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "param_dist = knn.cv_param_dist\n",
    "estimator = knn.estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(param_dist.param_grid)\n",
    "print(\"number of grids = %i\" % len(param_dist.param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for grid in param_dist.param_grid:\n",
    "    grid_search = RandomizedSearchCV(estimator, param_distributions=grid, n_iter=2, scoring='accuracy',\n",
    "                           cv=10, n_jobs=-1, verbose=0, error_score=0, return_train_score=True)\n",
    "    #grid_search = GridSearchCV(estimator, param_grid=param_dist.param_grid, scoring='accuracy',\n",
    "    #                       cv=10, n_jobs=-1, verbose=0, error_score=0, return_train_score=True)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(\"Best mean score: %.4f\" % (grid_search.best_score_))\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    print(\"Accuracy on test set: %.4f\" % accuracy_score(y_test, grid_search.best_estimator_.predict(X_test))) \n",
    "    results.append((grid_search.best_score_, grid_search.best_estimator_))\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import classifiers\n",
    "clfs = classes.Classifiers(verbose=0, method='complete', num_clf=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting and tuning of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a way of optimizing the hyper parameters of each selected algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular fitting (just train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the basic building block: \"fit_classifiers\" loops over each algorithm and trains on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit classifiers on data\n",
    "clfs.fit_classifiers(X_train,y_train,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune hyper parameters using cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scoring options:\n",
    "* ['accuracy', 'adjusted_rand_score', 'average_precision', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_median_absolute_error', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete and categorical variables (using scipy.stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "scorer = make_scorer(log_loss, greater_is_better=False)\n",
    "scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def neg_log_loss(y_true, y_pred):\n",
    "    return -1.0 * log_loss(y_true, y_pred)\n",
    "scorer = make_scorer(neg_log_loss)\n",
    "scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {\"max_depth\": [1, None],\n",
    "              \"max_features\": sp_randint(1, 50),\n",
    "              \"min_samples_split\": sp_randint(1, 50),\n",
    "              \"min_samples_leaf\": sp_randint(1, 50),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"],\n",
    "              \"n_estimators\": [100, 200, 300]} \n",
    "estimator = RandomForestClassifier()\n",
    "grid_search = RandomizedSearchCV(estimator, param_distributions=param_dist, n_iter=25, scoring='accuracy',\n",
    "                       cv=5, n_jobs=-1, verbose=0, error_score=0, return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "#pd.DataFrame(grid_search.cv_results_).head()\n",
    "print(\"Best mean score: %.4f\" % (grid_search.best_score_))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy on test set: %.4f\" % accuracy_score(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous variable (using scipy.stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import uniform, gamma, lognorm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "param_dist = {\"C\": uniform(loc=0., scale=200.),\n",
    "              \"fit_intercept\": [True, False],\n",
    "              \"penalty\": ['l1', 'l2'],\n",
    "              \"max_iter\": [50, 100, 150]}\n",
    "estimator = LogisticRegression()\n",
    "grid_search = RandomizedSearchCV(estimator, param_distributions=param_dist, n_iter=200, scoring='accuracy',\n",
    "                       cv=10, n_jobs=-1, verbose=0, error_score=0, return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best mean score: %.4f\" % (grid_search.best_score_))\n",
    "print(grid_search.best_params_)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy on test set: %.4f\" % accuracy_score(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict on data (returns a list of (name, prediction) tuples for each classifier in repository)\n",
    "preds = clfs.predict_classifiers(X_train)\n",
    "clfs.verbose = 1 # Report progress if =1, else be quiet if =0.\n",
    "# How well are we doing?\n",
    "scores = clfs.classifier_performance(preds, y_train, metric='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = clfs.predict_classifiers(X_test)\n",
    "clfs.verbose = 1 # Report progress if =1, else be quiet if =0.\n",
    "# How well are we doing?\n",
    "scores = clfs.classifier_performance(preds, y_test, metric='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an instance \"correlation checking\" classes\n",
    "import class_handler as classes; reload(classes)\n",
    "check = classes.CheckClassifierCorrelation(prediction_type='c_binary')\n",
    "# Get names and correlation matrix\n",
    "names_, corr_ = check.compute_correlation_matrix(preds)\n",
    "# Plot correlation matrix (output depends on if plotting inline or not)\n",
    "fig = check.plot_correlation_matrix(names_, corr_, fig_size=(16,10), rot=20, font_scale=1.1, file=os.getcwd()+\"/corr.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
